"""
OPTIMIZED HYBRID MODEL TRAINING PIPELINE
========================================
Retrains YOLO with better parameters, optimizes FracNet, and fine-tunes ensemble weights
Target: 90%+ accuracy
"""

import os
import sys
import torch
import numpy as np
from pathlib import Path
import json
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
import seaborn as sns
from datetime import datetime

# Add src directory to path
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

def train_optimized_yolo():
    """
    Retrain YOLO with optimized parameters for medical imaging
    """
    print("üîÑ RETRAINING YOLO WITH OPTIMIZED PARAMETERS...")
    
    from ultralytics import YOLO
    
    # Initialize with pre-trained weights
    model = YOLO('yolo11n.pt')
    
    # Optimized training parameters for medical images
    training_config = {
        'data': r'C:\Users\Narendar\OneDrive\Desktop\bone-fracture-hybrid\dataset\dataset\data.yaml',
        'epochs': 5,  # Optimized for 5 epochs as requested for faster training
        'imgsz': 640,
        'batch': 8,  # Reduced for stability
        'lr0': 0.001,  # Lower learning rate for medical images
        'lrf': 0.01,   # Final learning rate
        'momentum': 0.937,
        'weight_decay': 0.0005,
        'warmup_epochs': 3,
        'warmup_momentum': 0.8,
        'warmup_bias_lr': 0.1,
        'box': 7.5,    # Box loss gain
        'cls': 0.5,    # Classification loss gain
        'dfl': 1.5,    # DFL loss gain
        'pose': 12.0,  # Pose loss gain
        'kobj': 1.0,   # Keypoint objectness loss gain
        'label_smoothing': 0.0,
        'nbs': 64,     # Nominal batch size
        'hsv_h': 0.015,  # HSV hue augmentation
        'hsv_s': 0.7,    # HSV saturation augmentation
        'hsv_v': 0.4,    # HSV value augmentation
        'degrees': 0.0,  # Rotation augmentation (disabled for medical)
        'translate': 0.1, # Translation augmentation
        'scale': 0.5,    # Scale augmentation
        'shear': 0.0,    # Shear augmentation (disabled)
        'perspective': 0.0, # Perspective augmentation (disabled)
        'flipud': 0.0,   # Vertical flip (disabled for medical)
        'fliplr': 0.5,   # Horizontal flip
        'mosaic': 1.0,   # Mosaic augmentation
        'mixup': 0.0,    # Mixup augmentation
        'copy_paste': 0.0, # Copy-paste augmentation
        'auto_augment': 'randaugment',
        'erasing': 0.4,  # Random erasing
        'crop_fraction': 1.0,
        'save_period': 5,  # Save checkpoint every 5 epochs
        'project': 'src/yolo/runs/fracture_classification',
        'name': 'yolo_optimized_v1',
        'exist_ok': True,
        'pretrained': True,
        'optimizer': 'AdamW',  # Better optimizer for medical images
        'verbose': True,
        'seed': 0,
        'deterministic': True,
        'single_cls': False,
        'rect': False,
        'cos_lr': True,  # Cosine learning rate scheduler
        'close_mosaic': 10,  # Close mosaic augmentation in last 10 epochs
        'resume': False,
        'amp': True,  # Automatic mixed precision
        'fraction': 1.0,
        'profile': False,
        'freeze': None,
        'multi_scale': True,  # Multi-scale training
        'overlap_mask': True,
        'mask_ratio': 4,
        'dropout': 0.0,
        'val': True,
        'plots': True,
        'save': True,
        'save_json': True,
        'save_hybrid': False,
        'conf': None,
        'iou': 0.7,
        'max_det': 300,
        'half': False,
        'dnn': False,
        'augment': False,
        'agnostic_nms': False,
        'retina_masks': False,
        'format': 'torchscript',
        'keras': False,
        'optimize': False,
        'int8': False,
        'dynamic': False,
        'simplify': False,
        'opset': None,
        'workspace': 4,
        'nms': False,
        'embed': None,
    }
    
    print(f"Training with {training_config['epochs']} epochs...")
    print(f"Batch size: {training_config['batch']}")
    print(f"Learning rate: {training_config['lr0']}")
    print(f"Optimizer: {training_config['optimizer']}")
    
    # Train the model
    results = model.train(**training_config)
    
    # Save optimized model
    model.save('src/yolo/weights/yolo11n_optimized_v1.pt')
    
    print("‚úÖ YOLO OPTIMIZATION COMPLETE!")
    print(f"Results: {results}")
    
    return model, results

def train_optimized_fracnet():
    """
    Optimize FracNet training with better parameters
    """
    print("üîÑ OPTIMIZING FRACNET TRAINING...")
    
    # Import custom FracNet
    from src.custom_fracnet.train_custom_fracnet2d import train_fracnet_optimized
    
    # Optimized training parameters
    fracnet_config = {
        'epochs': 5,  # Optimized for 5 epochs as requested for faster training
        'batch_size': 16,  # Optimal for medical images
        'learning_rate': 0.0005,  # Lower for stability
        'weight_decay': 1e-4,
        'dropout': 0.3,  # Regularization
        'early_stopping_patience': 8,
        'lr_scheduler': 'cosine',
        'data_augmentation': True,
        'class_weights': True,  # Handle class imbalance
        'gradient_clipping': 1.0,
        'mixed_precision': True,
        'warmup_epochs': 3,
        'temperature_scaling': True,  # For uncertainty quantification
        'label_smoothing': 0.1,
        'optimizer': 'AdamW',
        'beta1': 0.9,
        'beta2': 0.999,
        'eps': 1e-8,
        'amsgrad': True,
    }
    
    print(f"Training FracNet with {fracnet_config['epochs']} epochs...")
    print(f"Batch size: {fracnet_config['batch_size']}")
    print(f"Learning rate: {fracnet_config['learning_rate']}")
    
    # Train optimized FracNet
    model, history = train_fracnet_optimized(**fracnet_config)
    
    print("‚úÖ FRACNET OPTIMIZATION COMPLETE!")
    return model, history

def create_optimized_hybrid_model():
    """
    Create hybrid model with optimized weights
    """
    print("üîß CREATING OPTIMIZED HYBRID MODEL...")
    
    from src.enhanced_hybrid_model import EnhancedHybridModel
    
    # Optimized ensemble weights based on component performance
    optimized_config = {
        'yolo_weight': 0.45,  # Increased after retraining
        'fracnet_weight': 0.55,  # Slightly reduced but still prioritized
        'consensus_bonus': 0.30,  # Increased bonus for agreement
        'high_confidence_bonus': 0.20,  # Higher bonus for confident predictions
        'consistency_bonus': 0.15,  # Reward consistency
        'uncertainty_penalty': 0.10,  # Penalty for high uncertainty
        
        # Optimized thresholds
        'yolo_threshold': 0.25,  # Adjusted for better sensitivity
        'fracnet_threshold': 0.30,  # Balanced threshold
        'consensus_threshold': 0.35,  # Decision threshold
        'high_confidence_threshold': 0.80,  # High confidence bar
        
        # Advanced fusion parameters
        'temperature_fracnet': 1.5,  # Temperature scaling for FracNet
        'temperature_yolo': 2.0,     # Temperature scaling for YOLO
        'adaptive_weights': True,     # Dynamic weight adjustment
        'uncertainty_weighting': True, # Uncertainty-based weighting
        'calibration': True,         # Probability calibration
        
        # Model paths
        'yolo_model_path': 'src/yolo/weights/yolo11n_optimized_v1.pt',
        'fracnet_model_path': 'src/custom_fracnet/weights/custom_fracnet2d_optimized.pth'
    }
    
    # Create enhanced hybrid model
    hybrid_model = EnhancedHybridModel(**optimized_config)
    
    print("‚úÖ OPTIMIZED HYBRID MODEL CREATED!")
    return hybrid_model, optimized_config

def fine_tune_ensemble_weights(hybrid_model, test_data):
    """
    Fine-tune ensemble weights using validation data
    """
    print("üéØ FINE-TUNING ENSEMBLE WEIGHTS...")
    
    best_weights = None
    best_accuracy = 0.0
    
    # Weight search space
    weight_ranges = {
        'yolo_weight': np.arange(0.2, 0.7, 0.05),
        'fracnet_weight': np.arange(0.3, 0.8, 0.05),
        'consensus_bonus': np.arange(0.1, 0.4, 0.05),
        'high_confidence_bonus': np.arange(0.1, 0.3, 0.05)
    }
    
    print("Searching optimal weight combinations...")
    
    best_results = []
    
    for yolo_w in weight_ranges['yolo_weight']:
        for fracnet_w in weight_ranges['fracnet_weight']:
            for consensus_b in weight_ranges['consensus_bonus']:
                for hc_bonus in weight_ranges['high_confidence_bonus']:
                    
                    # Ensure weights sum appropriately
                    if yolo_w + fracnet_w > 1.2:  # Allow some overlap for bonuses
                        continue
                    
                    # Update model weights
                    hybrid_model.yolo_weight = yolo_w
                    hybrid_model.fracnet_weight = fracnet_w
                    hybrid_model.consensus_bonus = consensus_b
                    hybrid_model.high_confidence_bonus = hc_bonus
                    
                    # Evaluate on test data
                    predictions = []
                    actuals = []
                    
                    for image_path, actual_label in test_data:
                        try:
                            result = hybrid_model.predict(image_path)
                            predictions.append(1 if result['is_fracture'] else 0)
                            actuals.append(actual_label)
                        except Exception as e:
                            print(f"Error processing {image_path}: {e}")
                            continue
                    
                    if len(predictions) > 0:
                        accuracy = accuracy_score(actuals, predictions)
                        
                        if accuracy > best_accuracy:
                            best_accuracy = accuracy
                            best_weights = {
                                'yolo_weight': yolo_w,
                                'fracnet_weight': fracnet_w,
                                'consensus_bonus': consensus_b,
                                'high_confidence_bonus': hc_bonus,
                                'accuracy': accuracy
                            }
                            
                        best_results.append({
                            'weights': {
                                'yolo_weight': yolo_w,
                                'fracnet_weight': fracnet_w,
                                'consensus_bonus': consensus_b,
                                'high_confidence_bonus': hc_bonus
                            },
                            'accuracy': accuracy
                        })
    
    # Sort results by accuracy
    best_results.sort(key=lambda x: x['accuracy'], reverse=True)
    
    print(f"‚úÖ BEST ACCURACY FOUND: {best_accuracy:.4f}")
    print(f"Optimal weights: {best_weights}")
    
    # Apply best weights
    if best_weights:
        hybrid_model.yolo_weight = best_weights['yolo_weight']
        hybrid_model.fracnet_weight = best_weights['fracnet_weight']
        hybrid_model.consensus_bonus = best_weights['consensus_bonus']
        hybrid_model.high_confidence_bonus = best_weights['high_confidence_bonus']
    
    return hybrid_model, best_weights, best_results[:10]  # Top 10 results

def comprehensive_evaluation(hybrid_model, test_data):
    """
    Comprehensive evaluation of the optimized hybrid model
    """
    print("üìä COMPREHENSIVE EVALUATION...")
    
    predictions = []
    actuals = []
    confidences = []
    decision_methods = []
    
    for image_path, actual_label in test_data:
        try:
            result = hybrid_model.predict(image_path)
            predictions.append(1 if result['is_fracture'] else 0)
            actuals.append(actual_label)
            confidences.append(result['confidence'])
            decision_methods.append(result.get('decision_method', 'unknown'))
        except Exception as e:
            print(f"Error processing {image_path}: {e}")
            continue
    
    if len(predictions) == 0:
        print("‚ùå No valid predictions made!")
        return None
    
    # Calculate metrics
    accuracy = accuracy_score(actuals, predictions)
    precision = precision_score(actuals, predictions, average='weighted', zero_division=0)
    recall = recall_score(actuals, predictions, average='weighted', zero_division=0)
    f1 = f1_score(actuals, predictions, average='weighted', zero_division=0)
    
    # Confusion matrix
    cm = confusion_matrix(actuals, predictions)
    
    results = {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'confusion_matrix': cm.tolist(),
        'total_samples': len(predictions),
        'avg_confidence': np.mean(confidences),
        'std_confidence': np.std(confidences),
        'decision_methods': dict(np.unique(decision_methods, return_counts=True))
    }
    
    print(f"üìà FINAL RESULTS:")
    print(f"   Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"   Precision: {precision:.4f}")
    print(f"   Recall: {recall:.4f}")
    print(f"   F1-Score: {f1:.4f}")
    print(f"   Total Samples: {len(predictions)}")
    print(f"   Average Confidence: {np.mean(confidences):.4f}")
    
    # Save results
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = f"evaluation_results/optimized_hybrid_results_{timestamp}.json"
    
    os.makedirs("evaluation_results", exist_ok=True)
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    # Plot confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['No Fracture', 'Fracture'],
                yticklabels=['No Fracture', 'Fracture'])
    plt.title(f'Optimized Hybrid Model - Confusion Matrix\nAccuracy: {accuracy:.4f}')
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.tight_layout()
    plt.savefig(f'evaluation_results/optimized_confusion_matrix_{timestamp}.png', dpi=300)
    plt.close()
    
    return results

def load_test_dataset():
    """Load test dataset with proper labels"""
    test_data = []
    test_dir = Path(r"C:\Users\Narendar\OneDrive\Desktop\bone-fracture-hybrid\dataset\dataset\test")
    
    if not test_dir.exists():
        print(f"‚ùå Test directory not found: {test_dir}")
        return test_data
    
    # Load images and labels
    for img_file in test_dir.glob("*.jpg"):
        label_file = test_dir / "labels" / f"{img_file.stem}.txt"
        
        if label_file.exists():
            with open(label_file, 'r') as f:
                lines = f.readlines()
                # Check if any line contains class 1 (fracture)
                has_fracture = any(line.strip().startswith('1 ') for line in lines)
                test_data.append((str(img_file), 1 if has_fracture else 0))
        else:
            # Default to no fracture if no label file
            test_data.append((str(img_file), 0))
    
    print(f"Loaded {len(test_data)} test samples")
    return test_data

def main():
    """
    Main optimization pipeline
    """
    print("üöÄ STARTING OPTIMIZED HYBRID MODEL TRAINING...")
    print("=" * 60)
    
    try:
        # Step 1: Retrain YOLO with optimized parameters
        yolo_model, yolo_results = train_optimized_yolo()
        
        # Step 2: Optimize FracNet training
        fracnet_model, fracnet_history = train_optimized_fracnet()
        
        # Step 3: Create optimized hybrid model
        hybrid_model, config = create_optimized_hybrid_model()
        
        # Step 4: Load test data
        test_data = load_test_dataset()
        if not test_data:
            print("‚ùå No test data available for optimization!")
            return
        
        # Step 5: Fine-tune ensemble weights
        optimized_hybrid, best_weights, top_results = fine_tune_ensemble_weights(hybrid_model, test_data)
        
        # Step 6: Comprehensive evaluation
        final_results = comprehensive_evaluation(optimized_hybrid, test_data)
        
        # Step 7: Save optimized configuration
        optimization_summary = {
            'timestamp': datetime.now().isoformat(),
            'yolo_optimization': str(yolo_results),
            'fracnet_optimization': 'completed',
            'best_ensemble_weights': best_weights,
            'top_weight_combinations': top_results,
            'final_performance': final_results,
            'target_achieved': final_results['accuracy'] >= 0.90 if final_results else False
        }
        
        with open('evaluation_results/optimization_summary.json', 'w') as f:
            json.dump(optimization_summary, f, indent=2)
        
        print("\n" + "=" * 60)
        print("üéâ OPTIMIZATION COMPLETE!")
        
        if final_results and final_results['accuracy'] >= 0.90:
            print(f"‚úÖ TARGET ACHIEVED: {final_results['accuracy']:.4f} (‚â•90%)")
        else:
            accuracy = final_results['accuracy'] if final_results else 0
            print(f"‚ö†Ô∏è  Target not fully met: {accuracy:.4f} (<90%)")
            print("   Consider further parameter tuning or data augmentation")
        
        print("üìÅ Results saved in evaluation_results/")
        
    except Exception as e:
        print(f"‚ùå Error during optimization: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()